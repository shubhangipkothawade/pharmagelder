{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Database Import\n",
    "* Update year\n",
    "* Update Query Injection (at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../../data/lib/')\n",
    "from consts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('../../data/3. transformation/7_hcp_grouped_final.csv')\n",
    "df1 = pd.read_csv('../../data/3. transformation/7_hco_grouped_final.csv')\n",
    "df_accumulation_raw = pd.read_csv('../../data/3. transformation/2_accumulations_cleaned.csv', dtype = dataframe_types)\n",
    "\n",
    "df_pharma_raw = pd.read_csv('sources/liste_companies.csv')\n",
    "df_pharma_source_raw = pd.read_csv('sources/pharma_source.csv')\n",
    "\n",
    "#Live Data\n",
    "df_pharma_live = pd.read_csv('../../data/0. live data/0_pharma.csv')\n",
    "df_pharma_source_live = pd.read_csv('../../data/0. live data/0_pharma_source.csv')\n",
    "\n",
    "plz_names = ['REC_ART', 'ONRP', 'BFSNR', 'PLZ_TYP', 'POSTLEITZAHL', 'PLZ_ZZ', 'GPLZ', 'ORTBEZ18', \n",
    "             'ORTBEZ27', 'KANTON', 'SPRACHCODE', 'SPRACHCODE_ABW', 'BRIEFZ_DURCH', 'GILT_AB_DAT', 'PLZ_BRIEFZUST', 'PLZ_COFF']\n",
    "df_plz_raw = pd.read_csv('sources/Post_Adressdaten20190122_edited.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove hcp_count and hco_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accumulation_raw = df_accumulation_raw[df_accumulation_raw.type.isin(['hcp_amount', 'hco_amount', 'rnd'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIX IT! Set Country to CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['country'] = 'CH'\n",
    "df1['country'] = 'CH'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find next recipient ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next free ID: 12871\n"
     ]
    }
   ],
   "source": [
    "df_recipient_live = pd.read_csv('../../data/0. live data/0_recipient.csv')\n",
    "recipient_id = df_recipient_live['id'].max() + 1\n",
    "print(\"Next free ID: %s\" % recipient_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Group Key\n",
    "HCO and HCP both have same indexes. Generate unique values for the address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcp = df0.copy()\n",
    "df_hco = df1.copy()\n",
    "\n",
    "group_key = recipient_id\n",
    "\n",
    "def set_groupkey(dataframe, startkey):\n",
    "    \n",
    "    dataframe['group_key'] = -1\n",
    "        \n",
    "    #Add Key to all without parent (-1) and without a group (-1)\n",
    "    for index, g in dataframe[(dataframe.parent == -1) & (dataframe.group == -1)].iterrows():\n",
    "        dataframe.loc[index, 'group_key'] = startkey\n",
    "        startkey += 1\n",
    "\n",
    "    #Add Key to groups\n",
    "    for g in dataframe[dataframe.group != -1]['group'].unique():\n",
    "        dataframe.loc[dataframe['group'] == g, 'group_key'] = startkey\n",
    "        startkey += 1\n",
    "        #print(g)\n",
    "\n",
    "\n",
    "    dataframe.drop(axis=1, columns=['group'])\n",
    "    return (dataframe, startkey)\n",
    "\n",
    "df_hcp, group_key = set_groupkey(df_hcp, group_key)\n",
    "df_hco, group_key = set_groupkey(df_hco, group_key)\n",
    "\n",
    "#Concat\n",
    "df_data = pd.concat([df_hcp, df_hco], sort=False)\n",
    "#df_data = pd.concat([df_hcp], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['year'] = 2019\n",
    "df_accumulation_raw['year'] = 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean\n",
    "df_data.drop(axis=1, columns=['group'], inplace=True)\n",
    "\n",
    "#Format\n",
    "df_data['group_key'] = df_data['group_key'].astype('int')\n",
    "\n",
    "#Reindex\n",
    "#df_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table `transaction_category`\n",
    "Create this table by hand. IDs have to be this way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_data = [\n",
    "    {\n",
    "        'trc_id': 1,\n",
    "        'trc_name': 'donations_grants'\n",
    "    },\n",
    "    {\n",
    "        'trc_id': 2,\n",
    "        'trc_name': 'sponsorship'\n",
    "    },\n",
    "    {\n",
    "        'trc_id': 3,\n",
    "        'trc_name': 'registration_fees'\n",
    "    },\n",
    "    {\n",
    "        'trc_id': 4,\n",
    "        'trc_name': 'travel_accommodation'\n",
    "    },\n",
    "    {\n",
    "        'trc_id': 5,\n",
    "        'trc_name': 'fees'\n",
    "    },\n",
    "    {\n",
    "        'trc_id': 6,\n",
    "        'trc_name': 'related_expenses'\n",
    "    },\n",
    "    {\n",
    "        'trc_id': 7,\n",
    "        'trc_name': 'rnd'\n",
    "    }\n",
    "]\n",
    "\n",
    "df_transaction_category =  pd.DataFrame(category_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table `transaction`\n",
    "\n",
    "If you have conversion errors, your data is not clean eq there are strings in value fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "\n",
    "def add_transaction(df_new, row, column, cat_id):\n",
    "    if not np.isnan(row[column]) and (row[column] != 0):\n",
    "        global c\n",
    "        c += 1\n",
    "        \n",
    "        tra_fk_recipient = row['parent']\n",
    "        if(row['parent'] == -1):\n",
    "            tra_fk_recipient = row['group_key']\n",
    "        \n",
    "        df_new = df_new.append({\n",
    "          #'tra_id': 0,\n",
    "          'tra_fk_pharma': row['pha_id'],\n",
    "          'tra_fk_recipient': tra_fk_recipient,\n",
    "          'tra_year': row['year'],\n",
    "          'tra_fk_transaction_category': cat_id,\n",
    "          'tra_value': row[column],\n",
    "          'tra_name_original': row['name'],\n",
    "          'tra_location_original': row['location'], \n",
    "          'tra_address_original': row['address']\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    return df_new\n",
    "\n",
    "#Create empty dataframe\n",
    "col_names =  [\n",
    "              #'tra_id', \n",
    "              'tra_fk_pharma',\n",
    "              'tra_fk_recipient',\n",
    "              'tra_year',\n",
    "              'tra_fk_transaction_category',\n",
    "              'tra_value',\n",
    "              'tra_name_original',\n",
    "              'tra_location_original', \n",
    "              'tra_address_original'\n",
    "            ]\n",
    "\n",
    "df_transaction = pd.DataFrame(columns = col_names)\n",
    "\n",
    "#Copy dataframe\n",
    "df_data_temp = df_data.copy()\n",
    "\n",
    "#Join with pharma\n",
    "df_data_temp = df_data_temp.merge(right = df_pharma_raw, how='left', left_on='source', right_on='pha_key')\n",
    "\n",
    "#Prepare transaction category id\n",
    "cat_donations_grants = df_transaction_category.loc[df_transaction_category.trc_name == 'donations_grants', 'trc_id'].iloc[0]\n",
    "cat_sponsorship = df_transaction_category.loc[df_transaction_category.trc_name == 'sponsorship', 'trc_id'].iloc[0]\n",
    "cat_registration_fees = df_transaction_category.loc[df_transaction_category.trc_name == 'registration_fees', 'trc_id'].iloc[0]\n",
    "cat_travel_accommodation = df_transaction_category.loc[df_transaction_category.trc_name == 'travel_accommodation', 'trc_id'].iloc[0]\n",
    "cat_fees = df_transaction_category.loc[df_transaction_category.trc_name == 'fees', 'trc_id'].iloc[0]\n",
    "cat_related_expenses = df_transaction_category.loc[df_transaction_category.trc_name == 'related_expenses', 'trc_id'].iloc[0]\n",
    "\n",
    "#Iter Addresses\n",
    "for index, row in df_data_temp.iterrows():\n",
    "    df_transaction = add_transaction(df_transaction, row, 'donations_grants', cat_donations_grants)\n",
    "    df_transaction = add_transaction(df_transaction, row, 'sponsorship', cat_sponsorship)\n",
    "    df_transaction = add_transaction(df_transaction, row, 'registration_fees', cat_registration_fees)\n",
    "    df_transaction = add_transaction(df_transaction, row, 'travel_accommodation', cat_travel_accommodation)\n",
    "    df_transaction = add_transaction(df_transaction, row, 'fees', cat_fees)\n",
    "    df_transaction = add_transaction(df_transaction, row, 'related_expenses', cat_related_expenses)\n",
    "\n",
    "#Set tra_id to index + 1\n",
    "#df_transaction['tra_id'] = df_transaction.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table `recipient`\n",
    "Import PLZ and create `zip_shadow` with all possible Zips for this location.  \n",
    "Source: https://www.post.ch/de/geschaeftlich/themen-a-z/adressen-pflegen-und-geodaten-nutzen/adress-und-geodaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recipient = df_data.copy()\n",
    "df_plz = df_plz_raw.copy()\n",
    "\n",
    "#Only Addresses withouth parent\n",
    "df_recipient = df_recipient[df_recipient.parent == -1]\n",
    "\n",
    "#Only main_address\n",
    "df_recipient.sort_values(by='main_address', inplace=True)\n",
    "df_recipient = df_recipient.groupby('group_key').first()\n",
    "\n",
    "#Remove year\n",
    "#df_recipient.drop(columns='_export_information', axis=1, inplace=True)\n",
    "df_recipient.drop(columns=['year', 'address_expand', 'location_expand', 'name_expand', 'parent', 'internalid'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "#Group PLZ by location\n",
    "df_plz['plz_shadow'] = df_plz['POSTLEITZAHL'].astype('str')\n",
    "df_plz['ORTBEZ18'] = df_plz['ORTBEZ18'].str.lower()\n",
    "df_plz = df_plz.groupby('ORTBEZ18')['plz_shadow'].agg(lambda col: ','.join(col))\n",
    "df_plz = df_plz.to_frame()\n",
    "\n",
    "#abstract index\n",
    "df_recipient['id'] = df_recipient.index\n",
    "\n",
    "#Lowercase location\n",
    "df_recipient['location_lower'] = df_recipient['location'].str.lower()\n",
    "\n",
    "#Join recipient and zip\n",
    "df_recipient = df_recipient.merge(right = df_plz, how='left', left_on='location_lower', right_on='ORTBEZ18')\n",
    "\n",
    "#covert plz\n",
    "df_recipient['plz'] = df_recipient['plz'].astype('int', errors='ignore')\n",
    "\n",
    "#Drop\n",
    "df_recipient.drop(axis=1, columns=['location_lower', 'main_address', 'donations_grants', 'sponsorship', 'registration_fees', 'travel_accommodation', 'fees', 'related_expenses', 'total', 'source'], inplace=True)\n",
    "\n",
    "#Add rec_zero_money = 0\n",
    "df_recipient['rec_zero_money'] = 0\n",
    "\n",
    "#Add rec_visible = true\n",
    "df_recipient['rec_visible'] = 1\n",
    "\n",
    "#rename\n",
    "df_recipient.columns = [\n",
    "        'rec_name',\n",
    "        'rec_location',\n",
    "        'rec_country',\n",
    "        'rec_address',\n",
    "        'rec_plz',\n",
    "        'rec_uci',\n",
    "        'rec_type',\n",
    "        'rec_id',\n",
    "        'rec_plz_shadow',\n",
    "        'rec_zero_money',\n",
    "        'rec_visible'\n",
    "    ]\n",
    "\n",
    "#Reorder Columns\n",
    "df_recipient = df_recipient[[\n",
    "        'rec_id',\n",
    "        'rec_name',\n",
    "        'rec_address',\n",
    "        'rec_location',\n",
    "        'rec_plz',\n",
    "        'rec_plz_shadow',\n",
    "        'rec_country',\n",
    "        'rec_uci',\n",
    "        'rec_zero_money',\n",
    "        'rec_visible',\n",
    "        'rec_type'\n",
    "    ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_recipient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table `accumulations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_accumulation(df_new, row, column):\n",
    "    if not np.isnan(row[column]) and (row[column] != 0):\n",
    "        \n",
    "        category = df_transaction_category.loc[df_transaction_category.trc_name == column, 'trc_id']\n",
    "        df_new = df_new.append({\n",
    "          #'acc_id': 0,\n",
    "          'acc_fk_pharma': row['pha_id'],\n",
    "          'acc_year': row['year'],\n",
    "          'acc_fk_transaction_category': category.iloc[0],\n",
    "          'acc_value': row[column],\n",
    "          'acc_type': row['type'],\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    return df_new\n",
    "\n",
    "#Create empty dataframe\n",
    "col_names =  [\n",
    "              #'acc_id', \n",
    "              'acc_fk_pharma',\n",
    "              'acc_year',\n",
    "              'acc_fk_transaction_category',\n",
    "              'acc_value',\n",
    "              'acc_type'\n",
    "            ]\n",
    "\n",
    "df_accumulation_new  = pd.DataFrame(columns = col_names)\n",
    "\n",
    "#Copy dataframe\n",
    "df_accumulation = df_accumulation_raw.copy()\n",
    "\n",
    "#Select amounts (no counts)\n",
    "df_accumulation = df_accumulation[df_accumulation['type'].isin(['hcp_amount', 'hco_amount', 'rnd'])]\n",
    "\n",
    "#Rename hcp_acount & hco_acmount\n",
    "df_accumulation['type'] = df_accumulation['type'].str.replace('_amount', '')\n",
    "\n",
    "#Join with pharma\n",
    "df_accumulation = df_accumulation.merge(right = df_pharma_raw, how='left', left_on='source', right_on='pha_key')\n",
    "\n",
    "#Iter Accumulations\n",
    "for index, row in df_accumulation.iterrows():\n",
    "    df_accumulation_new = add_accumulation(df_accumulation_new, row, 'donations_grants')\n",
    "    df_accumulation_new = add_accumulation(df_accumulation_new, row, 'sponsorship')\n",
    "    df_accumulation_new = add_accumulation(df_accumulation_new, row, 'registration_fees')\n",
    "    df_accumulation_new = add_accumulation(df_accumulation_new, row, 'travel_accommodation')\n",
    "    df_accumulation_new = add_accumulation(df_accumulation_new, row, 'fees')\n",
    "    df_accumulation_new = add_accumulation(df_accumulation_new, row, 'related_expenses')\n",
    "    \n",
    "\n",
    "#Add RnD\n",
    "for index, row in df_accumulation[df_accumulation.type == 'rnd'].iterrows():\n",
    "    if not np.isnan(row['total']) and (row['total'] != 0):\n",
    "        category = df_transaction_category.loc[df_transaction_category.trc_name == 'rnd', 'trc_id']\n",
    "        df_accumulation_new = df_accumulation_new.append({\n",
    "          #'acc_id': 0,\n",
    "          'acc_fk_pharma': row['pha_id'],\n",
    "          'acc_year': row['year'],\n",
    "          'acc_fk_transaction_category': category.iloc[0],\n",
    "          'acc_value': row['total'],\n",
    "          'acc_type': row['type'],\n",
    "        }, ignore_index=True)\n",
    "\n",
    "#Set acc_id to index + 1\n",
    "#df_accumulation_new['acc_id'] = df_accumulation_new.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table `pharma` and  `pharma_source`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Concat and remove duplicates\n",
    "df_pharma = pd.concat([df_pharma_live, df_pharma_raw], sort=False)\n",
    "df_pharma.drop_duplicates('pha_id', keep=False, inplace=True)\n",
    "\n",
    "#Add ID to pharma_source\n",
    "df_pharma_source = df_pharma_source_raw.merge(df_pharma, left_on='phs_key', right_on='pha_key')\n",
    "df_pharma_source = df_pharma_source[['pha_id', 'phs_source']]\n",
    "df_pharma_source.columns = ['phs_fk_pharma', 'phs_source']\n",
    "\n",
    "df_pharma.drop(columns='pha_key', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_transaction_category.to_csv('../../data/4. database/0_transaction_category.csv', index=False)\n",
    "df_pharma.to_csv('../../data/4. database/1_pharma.csv', index=False)\n",
    "df_pharma_source.to_csv('../../data/4. database/2_pharma_source.csv', index=False)\n",
    "df_recipient.to_csv('../../data/4. database/3_recipient.csv', index=False)\n",
    "df_transaction.to_csv('../../data/4. database/4_transaction.csv', index=False)\n",
    "df_accumulation_new.to_csv('../../data/4. database/5_accumulation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SQL Files\n",
    "Clean everything\n",
    "```sql\n",
    "DELETE FROM accumulation WHERE acc_id > 0;\n",
    "DELETE FROM pharma_source WHERE phs_id > 0;\n",
    "DELETE FROM pharma WHERE pha_id > 0;\n",
    "DELETE FROM recipient WHERE rec_id > 0;\n",
    "DELETE FROM transaction WHERE tra_id > 0;\n",
    "DELETE FROM transaction_category WHERE trc_id > 0;\n",
    "```\n",
    "Remove 2019\n",
    "```sql\n",
    "DELETE FROM transaction WHERE tra_year = 2019;\n",
    "DELETE FROM accumulation WHERE acc_year = 2019;\n",
    "DELETE FROM recipient WHERE rec_id >= 12871;\n",
    "DELETE FROM pharma WHERE pha_id = 61;\n",
    "DELETE FROM pharma WHERE pha_id = 62;\n",
    "DELETE FROM pharma_source WHERE phs_fk_pharma = 61;\n",
    "DELETE FROM pharma_source WHERE phs_fk_pharma = 62;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def export_sql(df, tablename, path):\n",
    "    #Columns\n",
    "    columns = list(map(lambda x: \"`\" + x + \"`\", df.columns))\n",
    "    columns = ', '.join(columns)\n",
    "\n",
    "    #Values\n",
    "    df = df.fillna(\"\")\n",
    "    rows = []\n",
    "    #qry\n",
    "    for row in df.values:\n",
    "        row_s = list(map(lambda x: \"'\" + str(x).replace(\"'\", \"\\\\'\") + \"'\", row))\n",
    "        row_s = '\\n(' +  ', '.join(row_s) + ')'\n",
    "        rows.append(row_s)\n",
    "\n",
    "    sublists  = list(chunks(rows, 3000))\n",
    "    inserts = []\n",
    "    for sub in sublists:\n",
    "        inserts.append('INSERT INTO %s (%s) VALUES %s;' % (tablename, columns, ', '.join(sub)))\n",
    "        \n",
    "    all_inserts = '\\n'.join(inserts)\n",
    "\n",
    "    #Safe File\n",
    "    text_file = open(path, \"w\", encoding='utf-8')\n",
    "    text_file.write(\"START TRANSACTION;\\n%s\\nCOMMIT;\\n\" % all_inserts)\n",
    "    text_file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export_sql(df_transaction_category, 'transaction_category', '../../data/4. database/0_transaction_category.sql')\n",
    "export_sql(df_pharma, 'pharma', '../../data/4. database/1_pharma.sql')\n",
    "export_sql(df_pharma_source, 'pharma_source', '../../data/4. database/2_pharma_source.sql')\n",
    "export_sql(df_recipient, 'recipient', '../../data/4. database/3_recipient.sql')\n",
    "export_sql(df_transaction, 'transaction', '../../data/4. database/4_transaction.sql')\n",
    "export_sql(df_accumulation_new, 'accumulation', '../../data/4. database/5_accumulation.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject manual Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat addresses\n",
    "# [[from, to]]\n",
    "concat = [\n",
    "    [10485, 10490],\n",
    "    [10749, 762],\n",
    "    [6720, 6719]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADD CONCATS!!!\n"
     ]
    }
   ],
   "source": [
    "q_list = []\n",
    "for r in concat:\n",
    "    #break # REMOVE!!!\n",
    "    q_update = 'UPDATE transaction SET tra_fk_recipient = %s WHERE tra_fk_recipient = %s;' % (r[1], r[0])\n",
    "    q_delete = 'DELETE FROM recipient WHERE rec_id = %s;' % r[0]\n",
    "    q_list.append(q_update)\n",
    "    q_list.append(q_delete)\n",
    "    \n",
    "#q_list.append('ALTER TABLE pharma DROP IF EXISTS pha_note;')\n",
    "#q_list.append('ALTER TABLE pharma ADD COLUMN IF NOT EXISTS pha_note TEXT NULL;')\n",
    "q_list.append('ALTER TABLE pharma ADD COLUMN pha_note TEXT NULL;')\n",
    "\n",
    "q_list.append(\"\"\"UPDATE pharma SET pha_note = '{\"de\": \"Seit 2019 weist Actelion Offenlegungen nicht mehr gesondert aus. Die Daten sind neu in den Veröffentlichungen der Firma Janssen-Cilag enthalten.\", \"fr\": \"Depuis 2019, les rapports de transparence d\\\\'Actelion ne sont plus publiés à part. Les données sont désormais intégrées dans les rapports de l\\\\'entreprise Janssen-Cilag.\"}' WHERE pha_id = 2;\"\"\")\n",
    "q_list.append(\"\"\"UPDATE pharma SET pha_note = '{\"de\": \"Seit 2019 weist Alcon Offenlegungen nicht mehr gesondert aus. Die Daten sind neu in den Veröffentlichungen der Firma Novartis enthalten.\", \"fr\": \"Depuis 2019, les rapports de transparence d\\\\'Alcon ne sont plus publiés à part. Les données sont désormais intégrées dans les rapports de l\\\\'entreprise Novartis.\"}' WHERE pha_id = 3;\"\"\")\n",
    "q_list.append(\"\"\"UPDATE pharma SET pha_note = '{\"de\": \"Basilea hat den Pharma-Kooperations-Kodex nicht unterzeichnet, Veröffentlichungen geschehen auf freiwilliger Basis. Für 2019 hat das Unternehmen noch keine Zahlen veröffentlicht.\", \"fr\": \"Basilea n\\\\'est pas signataire du Code de coopération pharmaceutique, ses publications de rapports se font sur une base volontaires. L\\\\'entreprise n\\\\'a pas communiqué de chiffres pour 2019.\"}' WHERE pha_id = 25;\"\"\")\n",
    "q_list.append(\"\"\"UPDATE pharma SET pha_note = '{\"de\": \"Seit 2019 weist Teva Offenlegungen nicht mehr gesondert aus. Die Daten sind neu in den Veröffentlichungen der Firma Mepha enthalten.\", \"fr\": \"Depuis 2019, les rapports de transparence de Teva ne sont plus publiés à part. Les données sont désormais intégrées dans les rapports de l\\\\'entreprise Mepha.\"}' WHERE pha_id = 55;\"\"\")\n",
    "\n",
    "q_list.append(\"UPDATE recipient SET rec_name = 'Novartis Pharma AG (als Empfänger)' WHERE rec_id = 10008;\")\n",
    "\n",
    "# Fix Typos\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Estavayer-Le-Lac' WHERE rec_id = 239\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Chêne-Bougeries' WHERE rec_id = 496\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Carouge' WHERE rec_id = 976\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Stans' WHERE rec_id = 1136\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'La Chaux-de-Fonds' WHERE rec_location = 'La Chaux-de- Fonds'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Baden' WHERE rec_location = 'Baden-Dättwil'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Basel' WHERE rec_location = 'Basel UniSpital'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Zürich' WHERE rec_location = 'Zuerich'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Le Grand-Saconnex' WHERE rec_location = 'Le Grand- Saconnex'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_name = 'Verbands Stadtzürcher Apotheker', rec_address = 'Rotbuchstrasse 830', rec_location = 'Zürich' WHERE rec_id = 14740;\")\n",
    "q_list.append(\"UPDATE recipient SET rec_address = 'Rotbuchstrasse 830', rec_location = 'Zürich' WHERE rec_id = 14400;\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Zürich' WHERE rec_id = 13222;\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Yverdon-Les-Bains' WHERE rec_location = 'Yverdon-Les- Bains'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Estavayer-Le-Lac' WHERE rec_location = 'Estavayer-Ie-Lac'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Frauenfeld' WHERE rec_location = 'Frauenfeid'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Wallisellen' WHERE rec_location = 'Glattzentrum'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Sainte-Croix' WHERE rec_location = 'Sainte Croix'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Neuchâtel' WHERE rec_location = 'Neuchätel'\")\n",
    "q_list.append(\"UPDATE recipient SET rec_location = 'Olten' WHERE rec_location = 'Oiten'\")\n",
    "\n",
    "\n",
    "with open('../../data/4. database/6_query_injections.sql', \"w\", encoding='utf-8') as text_file:\n",
    "    text_file.write(\"START TRANSACTION;\\n%s\\nCOMMIT;\\n\" % '\\n'.join(q_list))\n",
    "    text_file.close\n",
    "print(\"ADD CONCATS!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat files\n",
    "filenames = [#'../../data/4. database/0_transaction_category.sql',\n",
    "             '../../data/4. database/1_pharma.sql',\n",
    "             '../../data/4. database/2_pharma_source.sql',\n",
    "             '../../data/4. database/3_recipient.sql',\n",
    "             '../../data/4. database/4_transaction.sql',\n",
    "             '../../data/4. database/5_accumulation.sql',\n",
    "             '../../data/4. database/6_query_injections.sql',\n",
    "             ]\n",
    "with open('../../data/4. database/data_dump.sql', 'w', encoding='utf-8') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding='utf-8') as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
